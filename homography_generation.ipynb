{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e232e0f-84a3-4890-8d84-f89073a0a221",
   "metadata": {},
   "source": [
    "# Generate homography mapping for Crichton camera installation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395faaeb",
   "metadata": {},
   "source": [
    "Pair up keypoints from each camera to a farm position to generate a mapping between each camera and a global coordinate system.\n",
    "\n",
    "Below is a sample of the top-down mapping for a few of the cameras on the installation, showing areas with an overlap and how they interact.\n",
    "\n",
    "For convenience, the regularly-spaced farm landmarks are automatically converted from a `row,pillar` annotation to a global coordinate,\n",
    "\n",
    "- `(row=0, pillar=0)` is equivalent to global `(0, 0)`\n",
    "- `(row=2, pillar=7)` is equivalent to global `(120, 360)`\n",
    "\n",
    "The maximum dimension `(120, 360)` was chosen as this farm shed is `12m` by `36m`, so the global pixel coordinate is easily mentally translatable to a farm position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23264836",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(open(\"farm-layout.txt\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842e7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4470939d-b19d-4dc4-9a06-e850cbea5dc6",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1a22ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage import transform\n",
    "import homography as h\n",
    "from imgutil import crop_and_pad, plot_points_as_box, loadrgba, remove_box, remove_ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc116d82",
   "metadata": {},
   "source": [
    "## Pair up each image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e859eabb",
   "metadata": {},
   "source": [
    "# `def` warp_using_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9ceb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(image, matrix):\n",
    "    # Using scikit-image, warp the image using the matrix\n",
    "    # we need to add a slight border to the image to allow for edge areas to get transformed\n",
    "\n",
    "    image_new = transform.warp(image, matrix.inverse)\n",
    "\n",
    "    # Crop the image to the expected dimensions,\n",
    "    # so points that get transformed off the edge (as we are mapping floor area)\n",
    "    # do not extend the boundaries of the image\n",
    "    image_new = crop_and_pad(image_new, h.FARM_DIM)\n",
    "    return image_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52d7edd",
   "metadata": {},
   "source": [
    "# `def` Function which applies a homography mapping to a single coordinate point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_warp_coord(point, matrix):\n",
    "    \"\"\"Warp a single (x,y) point using a homography matrix.\n",
    "\n",
    "    This uses matrix multiplication to warp a 2D point.\n",
    "    - A z value of 1 is added to facilitate the 3D transform;\n",
    "    - this is then used to calculate the norm of the vector (z * [x; y; z]).\n",
    "    - The point is then multiplied by the matrix, and normalised using this.\n",
    "    - Finally, we slice the matrix to only return the warped [x, y] point.\n",
    "\n",
    "    This uses the '@' operator, introduced in Python 3.5, which calls\n",
    "    __matmul__ on the object.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    point : Tuple[int, int]\n",
    "    matrix : np.array\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    warped_point : Tuple[int, int]\n",
    "    \"\"\"\n",
    "    point = np.array([*point, 1])  # [x, y, 1]\n",
    "    norm = matrix[2] @ point\n",
    "    return np.ceil(matrix @ point / norm)[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737016e",
   "metadata": {},
   "source": [
    "# `def` helper to generate transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_using_points(filename, keypoints):\n",
    "    assert h.FARM_DIM\n",
    "    # Read in the image and correct the colour channel order\n",
    "    # as opencv loads BGR but matplotlib expects RGB\n",
    "    im = loadrgba(filename)\n",
    "\n",
    "    # Pillars in the farm are 5.25 metres apart, and the central reserve is\n",
    "    # 6m across the 12m shed\n",
    "    # Convert the pillar and row number into metres, and scale up to the desired\n",
    "    # farm dimension\n",
    "    keypoints_cam = []\n",
    "    keypoints_top = []\n",
    "    pix_w, pix_h = h.FARM_DIM\n",
    "    farm_w, farm_h = 7 * 5.25, 12\n",
    "\n",
    "    for kp in keypoints:\n",
    "        keypoints_cam.append(kp[\"camera\"])\n",
    "        xtop = kp[\"pillar\"] * 5.25 / farm_w * pix_w\n",
    "        ytop = kp[\"row\"] * 6 / farm_h * pix_h\n",
    "        keypoints_top.append((xtop, ytop))\n",
    "\n",
    "    # Estimate a transform that would match each keypoint from the camera view\n",
    "    # onto the top-down global perspective\n",
    "    keypoints_cam = np.array(keypoints_cam)\n",
    "    keypoints_top = np.array(keypoints_top)\n",
    "    matrix = transform.estimate_transform(\"projective\", keypoints_cam, keypoints_top)\n",
    "\n",
    "    im_top = transform_image(im, matrix)\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    #        Below is utility stuff for plotting and calculating error\n",
    "    # -----------------------------------------------------------------------\n",
    "    fs = 15\n",
    "    _, (axcam, axtop) = plt.subplots(nrows=2, figsize=(8, 8))\n",
    "\n",
    "    axcam.imshow(cv2.convertScaleAbs(im, alpha=1.4, beta=30))\n",
    "    axcam.grid()\n",
    "    axcam.scatter(keypoints_cam[:, 0], keypoints_cam[:, 1], c=\"r\", s=10)\n",
    "    for i, (x, y) in enumerate(keypoints_cam):\n",
    "        axcam.text(x * 1.01, y * 1.01, str(i + 1), fontsize=fs, c=\"red\")\n",
    "\n",
    "    axtop.imshow(im_top)\n",
    "    axtop.scatter(keypoints_top[:, 0], keypoints_top[:, 1], c=\"r\")\n",
    "    for i, (x, y) in enumerate(keypoints_top):\n",
    "        axtop.text(x * 1.01, y * 1.01, str(i + 1), fontsize=fs, c=\"r\")\n",
    "    axtop.set_xlim(0, 52.5*7)\n",
    "    axtop.set_ylim(120, 0)\n",
    "    plt.tight_layout(h_pad=0, w_pad=0)\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Calculate the error that happens during transformation by comparing\n",
    "    # the converted pixel coordinate to the target coordinate\n",
    "    #\n",
    "    pixel_x_err, pixel_y_err = [], []\n",
    "    for kp, kpt in zip(keypoints_cam, keypoints_top):\n",
    "        kpct = perspective_warp_coord(kp, matrix.params)\n",
    "        pixel_x_err.append(kpct[0] - kpt[0])\n",
    "        pixel_y_err.append(kpct[1] - kpt[1])\n",
    "    pixel_x_mean_err = np.mean(pixel_x_err)\n",
    "    pixel_x_std_err = np.std(pixel_x_err)\n",
    "    pixel_y_mean_err = np.mean(pixel_y_err)\n",
    "    pixel_y_std_err = np.std(pixel_y_err)\n",
    "\n",
    "    print(f\"Homography keypoint error, in pixels:\")\n",
    "    print(f\"\\tx {pixel_x_mean_err:.1f} mean {pixel_x_std_err:.1f} std\")\n",
    "    print(f\"\\ty {pixel_y_mean_err:.1f} mean {pixel_y_std_err:.1f} std\")\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c94b4a1",
   "metadata": {},
   "source": [
    "# Generate top-down mapping\n",
    "\n",
    "For each camera, we pass a list of `camera, row, pillar` dictionary items.\n",
    "\n",
    "| Key | Type | Description |\n",
    "|:--|:--|:--|\n",
    "| `camera` | `tuple[int, int]` | `(x, y)` image coordinate matching a farm landmark |\n",
    "| `pillar` | `int` in range `0..7` | numbered with `0` as camera 14 in the above diagram, and `7` aligned with camera 7 |\n",
    "| `row` | `int` in range `0..2` | indicating `0 right`, `1: middle`, `2: left`, as per the diagram above |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445072c5",
   "metadata": {},
   "source": [
    "## 01 left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = \"01left\"\n",
    "matrix_1left = warp_using_points(\n",
    "    f\"img/{cam}.png\",\n",
    "    keypoints=[\n",
    "        {'camera': (282, 312), 'row':0, 'pillar':1},\n",
    "        {'camera': (407, 190), 'row':0, 'pillar': 2},\n",
    "        {'camera': (466, 129), 'row':0, 'pillar': 3},\n",
    "        {'camera': (500, 96), 'row':0, 'pillar': 4},\n",
    "        {'camera': (520, 75), 'row':0, 'pillar': 5},\n",
    "        {'camera': (539, 49), 'row':0, 'pillar': 6.8},\n",
    "        {'camera': (630, 294), 'row':1, 'pillar':1},\n",
    "        {'camera': (620, 37), 'row':1, 'pillar': 6.8},\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac968c7",
   "metadata": {},
   "source": [
    "## 06 right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = \"06right\"\n",
    "matrix_6right = warp_using_points(\n",
    "    f\"img/{cam}.png\",\n",
    "    keypoints=[\n",
    "        {'camera': (75, 142), 'row': 0, 'pillar': 2.5},\n",
    "        {'camera': (170, 148), 'row': 0, 'pillar': 3},\n",
    "        {'camera': (255, 148), 'row': 0, 'pillar': 3.5},\n",
    "        {'camera': (383, 157), 'row': 0, 'pillar': 4.5},\n",
    "        {'camera': (434, 159), 'row': 0, 'pillar': 5},\n",
    "        {'camera': (543, 171), 'row': 0, 'pillar': 6.8},\n",
    "        {'camera': (135, 287), 'row': 1, 'pillar': 2.5},\n",
    "        {'camera': (271, 281), 'row': 1, 'pillar': 3},\n",
    "        {'camera': (464, 258), 'row': 1, 'pillar': 4},\n",
    "        {'camera': (550, 230), 'row': 1, 'pillar': 5},\n",
    "        {'camera': (599, 225), 'row': 1, 'pillar': 6},\n",
    "        {'camera': (623, 217), 'row': 1, 'pillar': 6.8},\n",
    "    ],\n",
    ")\n",
    "# del_bad_cam(cam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba969d76",
   "metadata": {},
   "source": [
    "## 13 right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95296391",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = \"13right\"\n",
    "matrix_13right = warp_using_points(\n",
    "    f\"img/{cam}.png\",\n",
    "    keypoints=[\n",
    "        {'camera': (622, 332), 'row': 0, 'pillar': 3},\n",
    "        {'camera': (533, 164), 'row': 1, 'pillar': 2},\n",
    "        {'camera': (434, 159), 'row': 1, 'pillar': 3},\n",
    "        {'camera': (207, 151), 'row': 1, 'pillar': 4},\n",
    "        {'camera': (550, 155), 'row': 1.2, 'pillar': 1.2},\n",
    "        {'camera': (525, 135), 'row': 1.4, 'pillar': 1.3},\n",
    "        {'camera': (544, 104), 'row': 2, 'pillar': 0},\n",
    "        {'camera': (424, 75), 'row': 2, 'pillar': 2},\n",
    "    ]\n",
    ")\n",
    "# del_bad_cam(cam)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d50562",
   "metadata": {},
   "source": [
    "## 15 left\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1657b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = \"15left\"\n",
    "matrix_15left = warp_using_points(\n",
    "    f\"img/{cam}.png\",\n",
    "    keypoints=[\n",
    "        {'camera': (46, 329), 'row': 0, 'pillar': 4},\n",
    "        {'camera': (495, 152), 'row': 1, 'pillar': 2.5},\n",
    "        {'camera': (350, 157), 'row': 1, 'pillar': 3},\n",
    "        {'camera': (186, 161), 'row': 1, 'pillar': 4},\n",
    "        {'camera': (108, 164), 'row': 1, 'pillar': 5},\n",
    "        {'camera': (58, 165), 'row': 1, 'pillar': 6},\n",
    "        {'camera': (37, 163), 'row': 1, 'pillar': 6.8},\n",
    "        {'camera': (111, 101), 'row': 2, 'pillar': 6.8},\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb42676",
   "metadata": {},
   "source": [
    "## Show transfer of a bounding box from camera to global view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb6b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_from_xywh(x, y, w, h):\n",
    "    # utility to convert xywh to a list of corner coordinates\n",
    "    return [(x, y), (x+w, y), (x+w, y+h), (x, y+h)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a68efc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img = loadrgba(\"img/15left.png\")\n",
    "mat = matrix_15left\n",
    "\n",
    "f, axes = plt.subplots(2, figsize=(8, 8), tight_layout=True)\n",
    "\n",
    "# Show an image of the camera, and a bounding box identifying one cow\n",
    "axes[0].imshow(cam_img)\n",
    "bb = box_from_xywh(190, 50, 90, 70)\n",
    "plot_points_as_box(bb, axes[0], color='red')\n",
    "\n",
    "# Transform the camera to it's top-down (global) view\n",
    "# and also transform each corner of the bounding box\n",
    "cam_top_img = transform_image(cam_img, mat)\n",
    "axes[1].imshow(cam_top_img)\n",
    "bbtop = [h.perspective_warp_coord(corner, mat.params) for corner in bb]\n",
    "plot_points_as_box(bbtop, axes[1], color='red')\n",
    "\n",
    "# Remove axes and tidy up the plot\n",
    "remove_box(axes[0])\n",
    "remove_box(axes[1])\n",
    "remove_ticks(axes[0])\n",
    "remove_ticks(axes[1])\n",
    "axes[1].set_xlim(0, 360)\n",
    "axes[1].set_ylim(120, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be0032f",
   "metadata": {},
   "source": [
    "# Transfer bounding box between views\n",
    "\n",
    "Now we have a global coordinate system, we can either transform points from each camera onto the top as a shared reference, or use the top view as an intermediate to transform points between each camera.\n",
    "\n",
    "We have an overlapping camera view betwen `06right` and `15left`, so we will use them as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10864011",
   "metadata": {},
   "source": [
    "## From each camera onto top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18c9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img = loadrgba(\"img/15left.png\")\n",
    "cam2_img = loadrgba(\"img/06right.png\")\n",
    "\n",
    "mat = matrix_15left\n",
    "mat2 = matrix_6right\n",
    "\n",
    "f = plt.figure(figsize=(8, 8), tight_layout=True)\n",
    "axTL = plt.subplot2grid((3, 2), (0, 0))\n",
    "axTR = plt.subplot2grid((3, 2), (0, 1))\n",
    "axML = plt.subplot2grid((3, 2), (1, 0))\n",
    "axMR = plt.subplot2grid((3, 2), (1, 1))\n",
    "axBOT = plt.subplot2grid((3, 2), (2, 0), colspan=2)\n",
    "\n",
    "# Show an image of the camera, and a bounding box identifying one cow\n",
    "axTL.imshow(cam_img)\n",
    "bb = box_from_xywh(190, 50, 90, 70)\n",
    "plot_points_as_box(bb, axTL, color='red')\n",
    "\n",
    "axTR.imshow(cam2_img)\n",
    "bb2 = box_from_xywh(540, 240, 60, 100)\n",
    "plot_points_as_box(bb2, axTR, color='blue')\n",
    "\n",
    "# Transform the camera to it's top-down (global) view\n",
    "# and also transform each corner of the bounding box\n",
    "cam_top_img = transform_image(cam_img, mat)\n",
    "cam2_top_img = transform_image(cam2_img, mat2)\n",
    "axML.imshow(cam_top_img)\n",
    "axMR.imshow(cam2_top_img)\n",
    "bbtop = [perspective_warp_coord(corner, mat.params) for corner in bb]\n",
    "bb2top = [perspective_warp_coord(corner, mat2.params) for corner in bb2]\n",
    "plot_points_as_box(bbtop, axML, color='red')\n",
    "plot_points_as_box(bb2top, axMR, color='blue')\n",
    "\n",
    "# Generate a transparent region showing the floormap of both cameras\n",
    "top_1 = np.zeros((120, 360, 4)).astype(int)\n",
    "xs, ys = np.where(cam_top_img[:, :, 3].astype(int))\n",
    "print(xs, ys)\n",
    "top_1[~xs, ~ys, 3] = 0\n",
    "top_1[xs, ys, 3] = 20\n",
    "top_1[xs, ys, 0] = 255\n",
    "\n",
    "top_2 = np.zeros((120, 360, 4)).astype(int)\n",
    "xs, ys = np.where(cam2_top_img[:, :, 3].astype(int))\n",
    "top_2[~xs, ~ys, 3] = 0\n",
    "top_2[xs, ys, 3] = 20\n",
    "top_2[xs, ys, 2] = 255\n",
    "\n",
    "axBOT.imshow(top_1.astype(int))\n",
    "axBOT.imshow(top_2.astype(int))\n",
    "plot_points_as_box(bbtop, axBOT, color='red')\n",
    "plot_points_as_box(bb2top, axBOT, color='blue')\n",
    "\n",
    "\n",
    "# Remove axes and tidy up the plot\n",
    "for ax in [axTL, axTR, axML, axMR, axBOT]:\n",
    "    remove_ticks(ax)\n",
    "\n",
    "for ax in [axML, axMR, axBOT]:\n",
    "    ax.set_xlim(0, 360)\n",
    "    ax.set_ylim(120, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf6236",
   "metadata": {},
   "source": [
    "## From one camera to another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fde1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_img = loadrgba(\"img/15left.png\")\n",
    "cam2_img = loadrgba(\"img/06right.png\")\n",
    "\n",
    "mat = matrix_15left\n",
    "mat2 = matrix_6right\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(nrows=2, figsize=(8, 8), tight_layout=True)\n",
    "\n",
    "# Show an image of the camera, and a bounding box identifying one cow\n",
    "ax1.imshow(cam_img)\n",
    "bb = box_from_xywh(250, 50, 60, 90)\n",
    "plot_points_as_box(bb, ax1, color='red')\n",
    "\n",
    "ax2.imshow(cam2_img)\n",
    "bb2 = box_from_xywh(420, 230, 130, 100)\n",
    "plot_points_as_box(bb2, ax2, color='blue')\n",
    "\n",
    "bbtop = [perspective_warp_coord(corner, mat.params) for corner in bb]\n",
    "bb2top = [perspective_warp_coord(corner, mat2.params) for corner in bb2]\n",
    "\n",
    "bb_to_bb2 = [perspective_warp_coord(corner, np.linalg.inv(mat2)) for corner in bbtop]\n",
    "bb2_to_bb = [perspective_warp_coord(corner, np.linalg.inv(mat)) for corner in bb2top]\n",
    "\n",
    "plot_points_as_box(bb2_to_bb, ax1, color='blue')\n",
    "plot_points_as_box(bb_to_bb2, ax2, color='red')\n",
    "\n",
    "# Remove axes and tidy up the plot\n",
    "for ax in [ax1, ax2]:\n",
    "    remove_ticks(ax)\n",
    "    ax.set_xlim(0, 640)\n",
    "    ax.set_ylim(360, 0)\n",
    "\n",
    "ax1.set_title('Red bounding box, and converted-from-view-2 Blue bounding box')\n",
    "ax2.set_title('Blue bounding box, and converted-from-view-1 Red bounding box')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
